{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una regresión lineal se deben cumplir ciertos criterios\n",
    "1. La linealidad: Debemos verificar que la relación entre varibles es lineal y la manera más fácil de hacerlo es graficar las variables independientes y dependiente en un diagrama de dispersión y si lo puntos dispersos mueustran una linea recta que se ajuste a los datos entonces cumple con el criterio. Lo que se puede hacer en caso de que no se cumpla es hacer otro tipo de regresión que no sea lineal o tranformar los datos para que se ajusten a la linealidad y esto se puede hacer a través de transformación exponencial o logaritmica\n",
    "2. No endogeneidad de los regresores: Ocurre cuando se omite una variable que es relevante para la regresión, ya que cada variable independiente explica 'y' y juantas se relacionan de alguna maneram entonces, al tratar de explicar los datos pueden llegar al error, esto también es conocido como sesgo de variable omitida. el sesgo de variable omitida ocurre cuando olvidas incluir una variable. Esto refleja en el término de error ya que el factor que olvidaste es incluido en el error. De esta manera, el error no es aleatorio pero incluye una parte sistemática (la variable sistemática)\n",
    "3. Normalidad y homocedastisidad: Asume que los residuos siguen una distribución normal y tienen una varianza constante en todos los niveles de las variables independientes. Las violaciones de esta suposición pueden afectar la precisión de las estimaciones de parámetros y las pruebas de hipótesis. Puede verificar la normalidad visualmente usando histogramas o gráficas de cuantil-cuantil (Q-Q) y evaluar la homocedasticidad usando gráficas de dispersión o pruebas estadísticas como las pruebas de Breusch-Pagan o White.\n",
    "4. No autocorrelación: En regrasion lineal, se puede detectar la correlacion al graficar los residuos, de modo que, si no hay patrones en la grafica, y por el contrario estan dispersos, entonces quiere decir que no hay autocorrelación. Otra prueba es la de Durbin Watson que se puede varificar en la tabla de resumen de resultados que nos provee statdmodels; generalmente su valor esta entre 0 y 4, 2 significa que no hay autocorrelación y los valores menores a 1 y mayores a 3 son causa de alarma. En este supuesto, no hay solución y lo mejor seria que cambies el modelo y no uses este cuando los terminos de error estan correlacionados.\n",
    "5. No multicolinealidad: Se refiere a una situación en la que dos o más variables independientes están linealmente relacionadas o exhiben un alto grado de correlación. La multicolinealidad puede dificultar la distinción de los efectos individuales de las variables independientes y puede dar lugar a estimaciones de coeficientes inestables o imprecisas. Puede evaluar la multicolinealidad utilizando matrices de correlación o medidas de diagnóstico como el factor de inflación de varianza (VIF)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
